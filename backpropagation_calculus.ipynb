{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPNwxNsx5zWM0VibUHerL3r",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ancestor9/2025_Winter_Deep-Learning-with-TensorFlow/blob/main/backpropagation_calculus.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. ìŠ¤ì»¬ë¼ì— ëŒ€í•˜ì—¬**\n",
        "\n",
        "- **ìˆ˜ì‹**:\n",
        "\n",
        "$$\n",
        "y = x^2\n",
        "$$\n",
        "\n",
        "- **ë¯¸ë¶„**:\n",
        "\n",
        "$$\n",
        "\\frac{dy}{dx} = 2x\n",
        "$$\n",
        "\n",
        "- **ê°’ ëŒ€ì…**:\n",
        "\n",
        "$$\n",
        "x = 2 \\;\\Rightarrow\\; \\text{gradient} = 4\n",
        "$$\n",
        "\n",
        "ğŸ‘‰ **`loss.backward()`ëŠ” ìˆ˜ì‹ì„ ë³´ê³  ë¯¸ë¶„ì‹ì„ ìë™ ê³„ì‚°í•œ ê²ƒ**\n",
        "\n"
      ],
      "metadata": {
        "id": "QA70UKDJcQ6R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# 1ï¸âƒ£ ë¯¸ë¶„ ëŒ€ìƒ ë³€ìˆ˜ ì„ ì–¸\n",
        "x = torch.tensor(2.0, requires_grad=True)\n",
        "\n",
        "# 2ï¸âƒ£ ê°„ë‹¨í•œ ìˆ˜ì‹ ì •ì˜\n",
        "y = x ** 2        # y = xÂ²\n",
        "\n",
        "# 3ï¸âƒ£ backward ì‹¤í–‰\n",
        "y.backward()     # xì— ëŒ€í•´ y( = xÂ²)ë¥¼ ë¯¸ë¶„í•œë‹¤\n",
        "\n",
        "# 4ï¸âƒ£ ê²°ê³¼ í™•ì¸\n",
        "print(x.grad)    # dy/dx = 2x = 4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TLzqJWZYcWGW",
        "outputId": "0e4d968c-76f0-4e39-ad4b-8a737943034d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(4.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. ì†ì‹¤í•¨ìˆ˜(Loss function)í˜•íƒœ ëŒ€í•˜ì—¬**\n",
        "\n",
        "- ëª¨ë¸: $y = wx$\n",
        "- ì†ì‹¤: $L = (wx - y)^2$\n",
        "- ë¯¸ë¶„ ê²°ê³¼: $\\frac{\\partial L}{\\partial w} = 2(wx - y)x$\n",
        "\n",
        "ğŸ‘‰ ì´ ê°’ì´ ìë™ìœ¼ë¡œ `w.grad`ì— ì €ì¥"
      ],
      "metadata": {
        "id": "80yUxD4hcZhM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# íŒŒë¼ë¯¸í„° (í•™ìŠµ ëŒ€ìƒ)\n",
        "w = torch.tensor(1.0, requires_grad=True)\n",
        "\n",
        "# ì…ë ¥ê³¼ ì •ë‹µ\n",
        "x = torch.tensor(3.0)\n",
        "y_true = torch.tensor(10.0)\n",
        "\n",
        "# ëª¨ë¸\n",
        "y_pred = w * x\n",
        "\n",
        "# ì†ì‹¤ í•¨ìˆ˜ (MSE)\n",
        "loss = (y_pred - y_true) ** 2\n",
        "print(loss)\n",
        "\n",
        "# ì—­ì „íŒŒ\n",
        "loss.backward()\n",
        "\n",
        "print(w.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxtRbGzUdVcz",
        "outputId": "c9a1953f-7a5c-47a8-ed73-eeb299aeec8a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(49., grad_fn=<PowBackward0>)\n",
            "tensor(-42.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"current w:\", w)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YoBY9VgFgCfn",
        "outputId": "165c753d-b541-4768-959a-5e3e35808374"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "current w: tensor(1., requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ê²½ì‚¬í•˜ê°•(gradient descent)**\n",
        "- **Forward â†’ loss ê³„ì‚° â†’ loss.backward() â†’ w.grad ìƒì„± â†’ w ì—…ë°ì´íŠ¸**\n",
        "\n",
        "> $w_{\\text{new}} = w_{\\text{old}} - \\alpha \\frac{\\partial L}{\\partial w}$\n",
        "\n",
        "> 1 - 0.01*(-42)"
      ],
      "metadata": {
        "id": "JHUy4_DDdV1B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "1 - 0.01*(-42)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rc9jPyPwgoYn",
        "outputId": "09a2edf9-22f0-4f0c-9ab2-1e4e2d713405"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.42"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.01\n",
        "\n",
        "with torch.no_grad():\n",
        "    w -= learning_rate * w.grad\n",
        "\n",
        "print(\"updated w:\", w)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptRT6u2yemr8",
        "outputId": "248c0332-eccd-4549-a895-6f460a7091a1"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "updated w: tensor(1.4200, requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# gradient ì´ˆê¸°í™”\n",
        "w.grad.zero_()\n",
        "\n",
        "print(\"updated w:\", w)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2qolUrZrf2bQ",
        "outputId": "ec49dafc-472b-46df-bd7e-203a850e14f4"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "updated w: tensor(1., requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\n",
        "\\mathbf{w}_{t+1}\n",
        "=\n",
        "\\mathbf{w}_t\n",
        "-\n",
        "\\alpha \\nabla_{\\mathbf{w}} L(\\mathbf{w}_t)\n",
        "$$\n"
      ],
      "metadata": {
        "id": "HgLMdhISen86"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\n",
        "\\mathbf{W}_{t+1}\n",
        "=\n",
        "\\mathbf{W}_t\n",
        "-\n",
        "\\alpha \\frac{\\partial L}{\\partial \\mathbf{W}}\n",
        "$$\n"
      ],
      "metadata": {
        "id": "J_-9l-lThTah"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. ë‹¨ìˆœíšŒê·€ì‹ ì ìš©**"
      ],
      "metadata": {
        "id": "Gz60BfZRkf_q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ\n",
        "# ==========================================\n",
        "import numpy as np                     # ìˆ˜ì¹˜ ê³„ì‚° ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
        "import torch                           # PyTorch í•µì‹¬ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
        "\n",
        "# í…ì„œ ì¶œë ¥ í¬ë§· ì„¤ì • (ê°€ë…ì„± ê°œì„ )\n",
        "torch.set_printoptions(edgeitems=2, linewidth=75)\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# 2. ë°ì´í„° ì •ì˜\n",
        "# ==========================================\n",
        "\n",
        "# t_c : ì‹¤ì œ ì¸¡ì •ëœ ì„­ì”¨ ì˜¨ë„ (ì •ë‹µê°’)\n",
        "t_c = [0.5, 14.0, 15.0, 28.0, 11.0, 8.0, 3.0, -4.0, 6.0, 13.0, 21.0]\n",
        "\n",
        "# t_u : ê´€ì¸¡ëœ í™”ì”¨ ì˜¨ë„ (ì…ë ¥ê°’)\n",
        "t_u = [35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4]\n",
        "\n",
        "# Python list â†’ PyTorch Tensor ë³€í™˜\n",
        "t_c = torch.tensor(t_c)\n",
        "t_u = torch.tensor(t_u)\n",
        "\n",
        "print(\"t_c shape:\", t_c.shape)\n",
        "print(\"t_u:\", t_u)\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# 3. ëª¨ë¸ ì •ì˜ (ì„ í˜• í•¨ìˆ˜)\n",
        "# ==========================================\n",
        "\n",
        "def model(t_u, w, b):\n",
        "    \"\"\"\n",
        "    ì„ í˜• íšŒê·€ ëª¨ë¸\n",
        "    t_u : ì…ë ¥ ë°ì´í„°\n",
        "    w   : ê°€ì¤‘ì¹˜(weight)\n",
        "    b   : í¸í–¥(bias)\n",
        "    \"\"\"\n",
        "    return w * t_u + b\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# 4. ì†ì‹¤ í•¨ìˆ˜ ì •ì˜\n",
        "# ==========================================\n",
        "\n",
        "def loss_fn(t_p, t_c):\n",
        "    \"\"\"\n",
        "    í‰ê·  ì œê³± ì˜¤ì°¨ (MSE)\n",
        "    t_p : ì˜ˆì¸¡ê°’\n",
        "    t_c : ì‹¤ì œê°’\n",
        "    \"\"\"\n",
        "    squared_diffs = (t_p - t_c) ** 2     # ì˜¤ì°¨ ì œê³±\n",
        "    return squared_diffs.mean()          # í‰ê· \n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# 5. íŒŒë¼ë¯¸í„° ì´ˆê¸°í™”\n",
        "# ==========================================\n",
        "\n",
        "# ìŠ¤ì¹¼ë¼ í…ì„œë¡œ ì´ˆê¸°í™”\n",
        "w = torch.ones((), requires_grad=True)  # ì´ˆê¸° ê°€ì¤‘ì¹˜ = 1\n",
        "b = torch.zeros((), requires_grad=True) # ì´ˆê¸° ì ˆí¸ = 0\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# 6. í•™ìŠµ ë£¨í”„\n",
        "# ==========================================\n",
        "\n",
        "learning_rate = 1e-2    # í•™ìŠµë¥ \n",
        "n_epochs = 5000         # ë°˜ë³µ íšŸìˆ˜\n",
        "\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "\n",
        "    # ----- (1) ìˆœì „íŒŒ -----\n",
        "    t_p = model(t_u, w, b)               # ì˜ˆì¸¡ê°’ ê³„ì‚°\n",
        "    loss = loss_fn(t_p, t_c)             # ì†ì‹¤ ê³„ì‚°\n",
        "\n",
        "    # ----- (2) ì—­ì „íŒŒ -----\n",
        "    loss.backward()                      # d(loss)/dw, d(loss)/db ê³„ì‚°\n",
        "\n",
        "    # ----- (3) íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ -----\n",
        "    with torch.no_grad():                # ê·¸ë˜í”„ ì¶”ì  ì¤‘ë‹¨\n",
        "        w -= learning_rate * w.grad\n",
        "        b -= learning_rate * b.grad\n",
        "\n",
        "    # print(f'wì™€ bì˜ ê°’ì€ : {w}, {b}')\n",
        "    # print('*'*100)\n",
        "\n",
        "    # ----- (4) gradient ì´ˆê¸°í™” -----\n",
        "    w.grad.zero_()      # ì´ ê²ƒì„  ì•ˆí•˜ë©´ ëˆ„ì í•¨\n",
        "    b.grad.zero_()\n",
        "    # print(f'wì™€ bì˜ ê°’ì€ : {w}, {b}')\n",
        "\n",
        "    # ----- (5) ë¡œê·¸ ì¶œë ¥ -----\n",
        "    if epoch % 1000 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss {loss.item():.4f}, w={w.item():.4f}, b={b.item():.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iW105lujhjeR",
        "outputId": "0fe302bf-46c5-4ece-94d4-6d887e598ab0"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "t_c shape: torch.Size([11])\n",
            "t_u: tensor([35.7000, 55.9000, 58.2000, 81.9000, 56.3000, 48.9000, 33.9000,\n",
            "        21.8000, 48.4000, 60.4000, 68.4000])\n",
            "Epoch 1000, Loss nan, w=nan, b=nan\n",
            "Epoch 2000, Loss nan, w=nan, b=nan\n",
            "Epoch 3000, Loss nan, w=nan, b=nan\n",
            "Epoch 4000, Loss nan, w=nan, b=nan\n",
            "Epoch 5000, Loss nan, w=nan, b=nan\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ”´ NaN(Not a Number) ë°œìƒ ì›ì¸ ë¶„ì„\n",
        "\n",
        "- **ê²°ë¡ **  \n",
        "  ê²°ê³¼ê°’ì´ `nan`ìœ¼ë¡œ ì¶œë ¥ëœ ì´ìœ ëŠ”  \n",
        "  **í•™ìŠµë¥ (learning_rate)ì´ ë„ˆë¬´ ë†’ì•„ ê°€ì¤‘ì¹˜ì™€ ì†ì‹¤ê°’ì´ ë°œì‚°í–ˆê¸° ë•Œë¬¸**ì´ë‹¤.\n",
        "\n",
        "---\n",
        "\n",
        "## 1ï¸âƒ£ í•™ìŠµë¥ ($1 \\times 10^{-2}$)ì˜ ê³¼ë„í•¨\n",
        "\n",
        "- **í˜„ìƒ**\n",
        "  - ì…ë ¥ ë°ì´í„° $t_u$ì˜ ê°’ ë²”ìœ„ê°€ **20 ~ 80**ìœ¼ë¡œ í° ìƒíƒœì—ì„œ\n",
        "  - í•™ìŠµë¥ ì„ $1e^{-2}$ë¡œ ì„¤ì •í•¨\n",
        "  - í•œ ë²ˆì˜ íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ ì‹œ ê°€ì¤‘ì¹˜ê°€ **ê³¼ë„í•˜ê²Œ ë³€í™”**\n",
        "\n",
        "- **ê²°ê³¼**\n",
        "  - ê¸°ìš¸ê¸°(Gradient)ê°€ ê¸‰ê²©íˆ ì»¤ì§\n",
        "  - íŒŒë¼ë¯¸í„° ê°’ì´ ì»´í“¨í„°ê°€ í‘œí˜„í•  ìˆ˜ ìˆëŠ” ë²”ìœ„ë¥¼ ì´ˆê³¼\n",
        "  - `inf (ë¬´í•œëŒ€)` â†’ `nan (Not a Number)`ë¡œ ë³€í™˜\n",
        "\n",
        "- **ìë£Œ ê·¼ê±°**\n",
        "  - ì²¨ë¶€ëœ ë…¸íŠ¸ë¶ **Page 3**\n",
        "  - í•™ìŠµë¥ ì´ $1e^{-2}$ì¼ ë•Œ  \n",
        "    **ë‹¨ 11 epoch ë§Œì— `Loss: inf` ë°œìƒ í™•ì¸**\n",
        "\n",
        "---\n",
        "\n",
        "## 2ï¸âƒ£ ê·¼ë³¸ ì›ì¸ ìš”ì•½\n",
        "\n",
        "- ì…ë ¥ ë°ì´í„°ì˜ **ìŠ¤ì¼€ì¼ì´ í° ìƒíƒœ**\n",
        "- í•™ìŠµë¥ ì´ **ì…ë ¥ ìŠ¤ì¼€ì¼ì— ë¹„í•´ ê³¼ë„**\n",
        "- Gradient í­ì£¼(Exploding Gradient) ë°œìƒ\n",
        "- ì†ì‹¤ê°’ê³¼ ê°€ì¤‘ì¹˜ê°€ ìˆ˜ì¹˜ì ìœ¼ë¡œ ë°œì‚°\n",
        "\n",
        "---\n",
        "\n",
        "## 3ï¸âƒ£ í•´ê²° ë°©ë²•: ë°ì´í„° ì •ê·œí™” (Normalization)\n",
        "\n",
        "- **ì ‘ê·¼ ë°©ë²•**\n",
        "  - ì…ë ¥ ë°ì´í„° $t_u$ì˜ ìŠ¤ì¼€ì¼ì„ ì¡°ì •\n",
        "  - ì˜ˆ:  \n",
        "    ```python\n",
        "    t_u = t_u * 0.1\n",
        "    ```\n",
        "\n",
        "- **íš¨ê³¼**\n",
        "  - Gradient í¬ê¸° ì•ˆì •í™”\n",
        "  - ì†ì‹¤ í•¨ìˆ˜ ì •ìƒ ìˆ˜ë ´\n",
        "  - `nan` ë¬¸ì œ í•´ê²°\n",
        "\n",
        "- **ë…¸íŠ¸ë¶ ì ìš© ì‚¬ë¡€**\n",
        "  - ì²¨ë¶€ëœ ë…¸íŠ¸ë¶ì—ì„œëŠ”  \n",
        "    **ì…ë ¥ ë°ì´í„° ìŠ¤ì¼€ì¼ë§ì„ í†µí•´ ë¬¸ì œë¥¼ í•´ê²°**í•˜ê³  ìˆìŒ\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ¯ í•µì‹¬ ìš”ì•½\n",
        "\n",
        "- `nan`ì€ ì½”ë“œ ì˜¤ë¥˜ê°€ ì•„ë‹ˆë¼ **ìˆ˜ì¹˜ì  ë¶ˆì•ˆì •ì„±ì˜ ì‹ í˜¸**\n",
        "- **ì…ë ¥ ìŠ¤ì¼€ì¼ â†” í•™ìŠµë¥ ì˜ ê· í˜•**ì´ í•µì‹¬\n",
        "- ë”¥ëŸ¬ë‹ì—ì„œ ë¬¸ì œ ë°œìƒ ì‹œ  \n",
        "  ğŸ‘‰ **ê°€ì¥ ë¨¼ì € ì…ë ¥ ë°ì´í„°ì™€ í•™ìŠµë¥ ì„ ì ê²€í•´ì•¼ í•¨**\n",
        "\n"
      ],
      "metadata": {
        "id": "YEpSqrZqlghu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------\n",
        "# ì…ë ¥ ìŠ¤ì¼€ì¼ ì¡°ì •\n",
        "# ----------------------------------\n",
        "t_u_norm = t_u * 0.1\n",
        "\n",
        "w = torch.ones((), requires_grad=True)\n",
        "b = torch.zeros((), requires_grad=True)\n",
        "\n",
        "learning_rate = 1e-2\n",
        "n_epochs = 5000\n",
        "\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "\n",
        "    t_p = model(t_u_norm, w, b)\n",
        "    loss = loss_fn(t_p, t_c)\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        w -= learning_rate * w.grad\n",
        "        b -= learning_rate * b.grad\n",
        "\n",
        "    w.grad.zero_()\n",
        "    b.grad.zero_()\n",
        "\n",
        "    if epoch % 1000 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss {loss.item():.4f}, w={w.item():.4f}, b={b.item():.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2Jn9L7nkHsA",
        "outputId": "5c65566b-8c55-48e9-d38b-cb8349881dfe"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1000, Loss 3.8285, w=4.8021, b=-14.1031\n",
            "Epoch 2000, Loss 2.9577, w=5.2644, b=-16.7200\n",
            "Epoch 3000, Loss 2.9286, w=5.3489, b=-17.1980\n",
            "Epoch 4000, Loss 2.9277, w=5.3643, b=-17.2853\n",
            "Epoch 5000, Loss 2.9276, w=5.3671, b=-17.3012\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "10InxYVkkLeh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}