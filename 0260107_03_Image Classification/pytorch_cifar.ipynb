{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ancestor9/2025_Winter_Deep-Learning-with-TensorFlow/blob/main/0260107_03_Image%20Classification/pytorch_cifar.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.packtpub.com/en-us/product/python-deep-learning-9781837638505"
      ],
      "metadata": {
        "id": "jajJL3lgagkK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll start by selecting device (GPU with fallback on CPU):"
      ],
      "metadata": {
        "id": "U3V-qqRyXWoI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "wAUUHRytXihG"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the training dataset `train_data` with data augmentation `train_transform` and batch loader `train_loader`:"
      ],
      "metadata": {
        "id": "SWTGPysXXru1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms as transforms\n",
        "from torchvision import datasets\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Training dataset\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomVerticalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        [0.485, 0.456, 0.406],\n",
        "        [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "train_data = datasets.CIFAR10(\n",
        "    root='data',\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=train_transform)\n",
        "\n",
        "batch_size = 50\n",
        "train_loader = DataLoader(\n",
        "    dataset=train_data,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9T_rRLlSX9zf",
        "outputId": "b9cfc30d-8483-49a9-f66e-15fcd0133d85"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:03<00:00, 44.8MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the validation dataset (no augmentations for validation):"
      ],
      "metadata": {
        "id": "U68QzudIYGUg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "validation_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        [0.485, 0.456, 0.406],\n",
        "        [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "validation_data = datasets.CIFAR10(\n",
        "    root='data',\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=validation_transform)\n",
        "\n",
        "validation_loader = DataLoader(\n",
        "    dataset=validation_data,\n",
        "    batch_size=100,\n",
        "    shuffle=True)"
      ],
      "metadata": {
        "id": "31ll_bZkYJj4"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the CNN model:"
      ],
      "metadata": {
        "id": "zIEpEHr0YMqw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1234)\n",
        "classes = 10\n",
        "\n",
        "from torch.nn import Sequential, Conv2d, BatchNorm2d, GELU, MaxPool2d, Dropout2d, Linear, Flatten\n",
        "\n",
        "model = Sequential(\n",
        "    Conv2d(in_channels=3, out_channels=32,\n",
        "           kernel_size=3, padding=1),\n",
        "    BatchNorm2d(32),\n",
        "    GELU(),\n",
        "    Conv2d(in_channels=32, out_channels=32,\n",
        "           kernel_size=3, padding=1),\n",
        "    BatchNorm2d(32),\n",
        "    GELU(),\n",
        "    MaxPool2d(kernel_size=2, stride=2),\n",
        "    Dropout2d(0.2),\n",
        "\n",
        "    Conv2d(in_channels=32, out_channels=64,\n",
        "           kernel_size=3, padding=1),\n",
        "    BatchNorm2d(64),\n",
        "    GELU(),\n",
        "    Conv2d(in_channels=64, out_channels=64,\n",
        "           kernel_size=3, padding=1),\n",
        "    BatchNorm2d(64),\n",
        "    GELU(),\n",
        "    MaxPool2d(kernel_size=2, stride=2),\n",
        "    Dropout2d(p=0.3),\n",
        "\n",
        "    Conv2d(in_channels=64, out_channels=128,\n",
        "           kernel_size=3),\n",
        "    BatchNorm2d(128),\n",
        "    GELU(),\n",
        "    Conv2d(in_channels=128, out_channels=128,\n",
        "           kernel_size=3),\n",
        "    BatchNorm2d(128),\n",
        "    GELU(),\n",
        "    MaxPool2d(kernel_size=2, stride=2),\n",
        "    Dropout2d(p=0.5),\n",
        "    Flatten(),\n",
        "\n",
        "    Linear(512, classes),\n",
        ")"
      ],
      "metadata": {
        "id": "ThA_DTuqYOgT"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the `train_model` function, which implements the training:"
      ],
      "metadata": {
        "id": "lBJZYtp-YUNK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, cost_function, optimizer, data_loader):\n",
        "    # send the model to the GPU\n",
        "    model.to(device)\n",
        "\n",
        "    # set model to training mode\n",
        "    model.train()\n",
        "\n",
        "    current_loss = 0.0\n",
        "    current_acc = 0\n",
        "\n",
        "    # iterate over the training data\n",
        "    for i, (inputs, labels) in enumerate(data_loader):\n",
        "        # send the input/labels to the GPU\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with torch.set_grad_enabled(True):\n",
        "            # forward\n",
        "            outputs = model(inputs)\n",
        "            _, predictions = torch.max(outputs, 1)\n",
        "            loss = cost_function(outputs, labels)\n",
        "\n",
        "            # backward\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # statistics\n",
        "        current_loss += loss.item() * inputs.size(0)\n",
        "        current_acc += torch.sum(predictions == labels.data)\n",
        "\n",
        "    total_loss = current_loss / len(data_loader.dataset)\n",
        "    total_acc = current_acc.double() / len(data_loader.dataset)\n",
        "\n",
        "    print('Train Loss: {:.4f}; Accuracy: {:.4f}'.format(total_loss, total_acc))"
      ],
      "metadata": {
        "id": "rI0V-_2SYX2h"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the `test_model` function, which implements the testing:"
      ],
      "metadata": {
        "id": "rK-qn3jgYaUp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model(model, cost_function, data_loader):\n",
        "    # send the model to the GPU\n",
        "    model.to(device)\n",
        "\n",
        "    # set model in evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    current_loss = 0.0\n",
        "    current_acc = 0\n",
        "\n",
        "    # iterate over  the validation data\n",
        "    for i, (inputs, labels) in enumerate(data_loader):\n",
        "        # send the input/labels to the GPU\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # forward\n",
        "        with torch.set_grad_enabled(False):\n",
        "            outputs = model(inputs)\n",
        "            _, predictions = torch.max(outputs, 1)\n",
        "            loss = cost_function(outputs, labels)\n",
        "\n",
        "        # statistics\n",
        "        current_loss += loss.item() * inputs.size(0)\n",
        "        current_acc += torch.sum(predictions == labels.data)\n",
        "\n",
        "    total_loss = current_loss / len(data_loader.dataset)\n",
        "    total_acc = current_acc.double() / len(data_loader.dataset)\n",
        "\n",
        "    print('Test Loss: {:.4f}; Accuracy: {:.4f}'.format(total_loss, total_acc))\n",
        "\n",
        "    return total_loss, total_acc"
      ],
      "metadata": {
        "id": "v_Lpo_AtYl6S"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the training and the validation for 50 epochs:"
      ],
      "metadata": {
        "id": "zMzf-PRBYnvO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cost_func = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "epochs = 50\n",
        "for epoch in range(epochs):\n",
        "    print('Epoch {}/{}'.format(epoch + 1, epochs))\n",
        "    train_model(model, cost_func, optimizer, train_loader)\n",
        "    test_model(model, cost_func, validation_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZCljBS4MYs-b",
        "outputId": "259dc90c-2c9a-485e-c93d-0e7f3e0b05c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "Train Loss: 1.6602; Accuracy: 0.3868\n",
            "Test Loss: 1.3239; Accuracy: 0.5134\n",
            "Epoch 2/50\n",
            "Train Loss: 1.3546; Accuracy: 0.5065\n",
            "Test Loss: 1.1069; Accuracy: 0.6020\n",
            "Epoch 3/50\n",
            "Train Loss: 1.2057; Accuracy: 0.5670\n",
            "Test Loss: 0.9729; Accuracy: 0.6523\n",
            "Epoch 4/50\n",
            "Train Loss: 1.1136; Accuracy: 0.6040\n",
            "Test Loss: 0.9101; Accuracy: 0.6736\n",
            "Epoch 5/50\n",
            "Train Loss: 1.0380; Accuracy: 0.6303\n",
            "Test Loss: 0.8619; Accuracy: 0.6938\n",
            "Epoch 6/50\n",
            "Train Loss: 0.9846; Accuracy: 0.6517\n",
            "Test Loss: 0.8176; Accuracy: 0.7049\n",
            "Epoch 7/50\n",
            "Train Loss: 0.9367; Accuracy: 0.6704\n",
            "Test Loss: 0.7619; Accuracy: 0.7264\n",
            "Epoch 8/50\n",
            "Train Loss: 0.8965; Accuracy: 0.6829\n",
            "Test Loss: 0.7231; Accuracy: 0.7444\n",
            "Epoch 9/50\n",
            "Train Loss: 0.8622; Accuracy: 0.6976\n",
            "Test Loss: 0.7238; Accuracy: 0.7472\n",
            "Epoch 10/50\n",
            "Train Loss: 0.8373; Accuracy: 0.7050\n",
            "Test Loss: 0.6973; Accuracy: 0.7524\n",
            "Epoch 11/50\n",
            "Train Loss: 0.8185; Accuracy: 0.7146\n",
            "Test Loss: 0.6535; Accuracy: 0.7724\n",
            "Epoch 12/50\n",
            "Train Loss: 0.7957; Accuracy: 0.7224\n",
            "Test Loss: 0.6417; Accuracy: 0.7741\n",
            "Epoch 13/50\n",
            "Train Loss: 0.7778; Accuracy: 0.7283\n",
            "Test Loss: 0.6246; Accuracy: 0.7789\n",
            "Epoch 14/50\n",
            "Train Loss: 0.7713; Accuracy: 0.7321\n",
            "Test Loss: 0.6364; Accuracy: 0.7753\n",
            "Epoch 15/50\n",
            "Train Loss: 0.7513; Accuracy: 0.7385\n",
            "Test Loss: 0.6177; Accuracy: 0.7812\n",
            "Epoch 16/50\n",
            "Train Loss: 0.7344; Accuracy: 0.7453\n",
            "Test Loss: 0.5935; Accuracy: 0.7911\n",
            "Epoch 17/50\n",
            "Train Loss: 0.7255; Accuracy: 0.7490\n",
            "Test Loss: 0.5992; Accuracy: 0.7908\n",
            "Epoch 18/50\n",
            "Train Loss: 0.7177; Accuracy: 0.7484\n",
            "Test Loss: 0.5891; Accuracy: 0.7929\n",
            "Epoch 19/50\n",
            "Train Loss: 0.7059; Accuracy: 0.7539\n",
            "Test Loss: 0.5824; Accuracy: 0.7958\n",
            "Epoch 20/50\n",
            "Train Loss: 0.6942; Accuracy: 0.7600\n",
            "Test Loss: 0.5958; Accuracy: 0.7885\n",
            "Epoch 21/50\n",
            "Train Loss: 0.6911; Accuracy: 0.7612\n",
            "Test Loss: 0.5586; Accuracy: 0.8032\n",
            "Epoch 22/50\n",
            "Train Loss: 0.6848; Accuracy: 0.7611\n",
            "Test Loss: 0.5577; Accuracy: 0.8049\n",
            "Epoch 23/50\n",
            "Train Loss: 0.6735; Accuracy: 0.7648\n",
            "Test Loss: 0.5454; Accuracy: 0.8064\n",
            "Epoch 24/50\n",
            "Train Loss: 0.6645; Accuracy: 0.7672\n",
            "Test Loss: 0.5488; Accuracy: 0.8068\n",
            "Epoch 25/50\n",
            "Train Loss: 0.6589; Accuracy: 0.7730\n",
            "Test Loss: 0.5387; Accuracy: 0.8121\n",
            "Epoch 26/50\n",
            "Train Loss: 0.6487; Accuracy: 0.7753\n",
            "Test Loss: 0.5321; Accuracy: 0.8123\n",
            "Epoch 27/50\n",
            "Train Loss: 0.6443; Accuracy: 0.7747\n",
            "Test Loss: 0.5370; Accuracy: 0.8128\n",
            "Epoch 28/50\n",
            "Train Loss: 0.6387; Accuracy: 0.7773\n",
            "Test Loss: 0.5416; Accuracy: 0.8095\n",
            "Epoch 29/50\n",
            "Train Loss: 0.6322; Accuracy: 0.7816\n",
            "Test Loss: 0.5263; Accuracy: 0.8166\n",
            "Epoch 30/50\n",
            "Train Loss: 0.6259; Accuracy: 0.7797\n",
            "Test Loss: 0.5266; Accuracy: 0.8158\n",
            "Epoch 31/50\n",
            "Train Loss: 0.6257; Accuracy: 0.7840\n",
            "Test Loss: 0.5189; Accuracy: 0.8210\n",
            "Epoch 32/50\n",
            "Train Loss: 0.6207; Accuracy: 0.7857\n",
            "Test Loss: 0.5171; Accuracy: 0.8178\n",
            "Epoch 33/50\n",
            "Train Loss: 0.6136; Accuracy: 0.7859\n",
            "Test Loss: 0.5314; Accuracy: 0.8156\n",
            "Epoch 34/50\n",
            "Train Loss: 0.6169; Accuracy: 0.7843\n",
            "Test Loss: 0.5189; Accuracy: 0.8191\n",
            "Epoch 35/50\n",
            "Train Loss: 0.6105; Accuracy: 0.7870\n",
            "Test Loss: 0.5189; Accuracy: 0.8208\n",
            "Epoch 36/50\n",
            "Train Loss: 0.6051; Accuracy: 0.7892\n",
            "Test Loss: 0.5225; Accuracy: 0.8172\n",
            "Epoch 37/50\n",
            "Train Loss: 0.5976; Accuracy: 0.7930\n",
            "Test Loss: 0.5058; Accuracy: 0.8244\n",
            "Epoch 38/50\n",
            "Train Loss: 0.5932; Accuracy: 0.7944\n",
            "Test Loss: 0.5080; Accuracy: 0.8236\n",
            "Epoch 39/50\n",
            "Train Loss: 0.5904; Accuracy: 0.7940\n",
            "Test Loss: 0.5018; Accuracy: 0.8251\n",
            "Epoch 40/50\n"
          ]
        }
      ]
    }
  ]
}